{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import pylab as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "#import librosa  #need install first\n",
    "#import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rulanxiao/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0,1,5,6,8,12,18,20,21,22,24,33,34,38,39,44,47,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/rulanxiao/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,11,13,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load metadata and features - change to local directory for tracks file\n",
    "tracks = pd.read_csv('/Users/rulanxiao/Desktop/fma_metadata/tracks.csv', header=None)\n",
    "echonest = pd.read_csv('/Users/rulanxiao/Desktop/fma_metadata/echonest.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names from \"track.1\", ... to named columns\n",
    "header = tracks.iloc[1]\n",
    "header[0] = 'track_ID'\n",
    "tracks.drop(tracks.index[[0,1,2]], inplace=True)\n",
    "tracks.rename(columns=header, inplace=True)\n",
    "tracks.head()\n",
    "\n",
    "header = echonest.iloc[2]\n",
    "header[0]='track_ID'\n",
    "echonest.drop(echonest.index[[0,1,2,3]],inplace=True)\n",
    "echonest.rename(columns = header,inplace=True)\n",
    "echonest.head()\n",
    "\n",
    "# Only select songs for which we have echonest data\n",
    "tracks_with_echonest_data = tracks[tracks['track_ID'].isin(echonest['track_ID'])]\n",
    "echonest_data = echonest[echonest['track_ID'].isin(tracks_with_echonest_data['track_ID'])]\n",
    "\n",
    "merged_echonest_data = pd.merge(tracks_with_echonest_data, echonest_data, on = 'track_ID')\n",
    "\n",
    "# Change duplicate \"listens\" column to track listens and album listens\n",
    "duplicate_listens = {'listens': ['album_listens', 'track_listens']}\n",
    "\n",
    "merged_echonest_data = merged_echonest_data.rename(columns=lambda c: duplicate_listens[c].pop(0) if c in duplicate_listens.keys() else c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year released from release date variable\n",
    "album_release_year = []\n",
    "\n",
    "for x in merged_echonest_data['date_released']:\n",
    "    if type(x)==str:\n",
    "        album_release_year.append(x[0:4])\n",
    "    else: \n",
    "        album_release_year.append(np.nan)\n",
    "\n",
    "#add album release year to dataframe\n",
    "merged_echonest_data['album_release_year'] = album_release_year\n",
    "\n",
    "# Check number of songs released per year and pick year with maximum\n",
    "import collections\n",
    "counter=collections.Counter(album_release_year)\n",
    "print(counter)\n",
    "print(counter.values())\n",
    "print(counter.keys())\n",
    "print(counter.most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For chosen year, check the number of songs released per genre; pick top three genres to use\n",
    "tracks_2010 = merged_echonest_data[merged_echonest_data['album_release_year'] == '2010']\n",
    "\n",
    "# Top genres for 2010\n",
    "counter=collections.Counter(tracks_2010['genre_top'])\n",
    "print(counter)\n",
    "print(counter.values())\n",
    "print(counter.keys())\n",
    "print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment analysis by title into dataset - compound score of positive / negative sentiment for song title\n",
    "# Rulan\n",
    "# require to install nltk first\n",
    "# this part work nice, file will be generated to our project folder. \n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Read in track data - change to local directory\n",
    "tracks_senti = pd.read_csv('tracks.csv',header=None)\n",
    "\n",
    "header = tracks_senti.iloc[1]\n",
    "header[0]='track ID'\n",
    "header[52]='track title'\n",
    "tracks_senti.drop(tracks_senti.index[[0,1,2]],inplace=True)\n",
    "tracks_senti.rename(columns = header,inplace=True)\n",
    "tracks_senti.head()\n",
    "\n",
    "df=tracks_senti[['track ID','track title']]\n",
    "df.dropna(axis=0, how='any')\n",
    "\n",
    "ml = df[\"track title\"].values\n",
    "title=[]\n",
    "for i in range(len(ml)):\n",
    "    a=str(ml[i])\n",
    "    title.append(a)\n",
    "idd = df[\"track ID\"].values\n",
    "\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "comp=[]\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in title:\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    score=[]\n",
    "    for k in ss:\n",
    "        a=ss[k]\n",
    "        score.append(a)\n",
    "    neg.append(score[0])\n",
    "    neu.append(score[1])\n",
    "    pos.append(score[2])\n",
    "    comp.append(score[3])\n",
    "\n",
    "sentimentall = pd.DataFrame({'track_ID':idd,'track_title':title,'senti neg': neg,'senti neu': neu,'senti pos': pos,'senti comp': comp})\n",
    "\n",
    "sentimentall.to_csv('sentimental_analysis_title.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment to dataframe\n",
    "sentiment= pd.read_csv('sentimental_analysis_title.csv')\n",
    "sentiment=sentiment[['track_ID','senti comp', 'senti neg','senti pos']]\n",
    "tracks_2010 = pd.merge(tracks_2010, sentiment, how='inner', on=['track_ID'])\n",
    "tracks_2010.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select the variables we want - named variables \n",
    "t10 = tracks_2010[['track_ID','latitude','longitude','bit_rate','duration','genre_top','track_listens','acousticness','danceability','energy','instrumentalness','liveness','speechiness','tempo','valence','artist_hotttnesss','artist_discovery','artist_familiarity', 'senti neg','senti pos']]\n",
    "t10['popular'] = (t10['track_listens']>=1147.25).astype(int)\n",
    "# For location (latitude, longitude), if na, change to 0\n",
    "t10['latitude'].fillna(0, inplace=True)\n",
    "t10['longitude'].fillna(0, inplace=True)\n",
    "\n",
    "# For genre-specific models - split dataset into three based on top three genres\n",
    "Hiphop_10=t10.loc[t10['genre_top'] == 'Hip-Hop']\n",
    "Rock_10=t10.loc[t10['genre_top'] == 'Rock']\n",
    "Elec_10=t10.loc[t10['genre_top'] == 'Electronic']\n",
    "\n",
    "# For baseline model - add genre as a categorical variable \n",
    "t10['Rock'] = (t10['genre_top'] == 'Rock').astype(int)\n",
    "t10['Hip-Hop'] = (t10['genre_top'] == 'Hip-Hop').astype(int)\n",
    "t10['Electronic'] = (t10['genre_top'] == 'Electronic').astype(int)\n",
    "t10.drop(['genre_top'], axis=1, inplace=True)\n",
    "\n",
    "t10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take hip-hop, rock, and hip-hop together\n",
    "t10['g']=t10['Rock']+ t10['Hip-Hop'] +t10['Electronic']\n",
    "t10=t10.loc[t10['g'] == 1]\n",
    "t10.drop(['g'],axis=1,inplace=True)\n",
    "\n",
    "# only genres Hiphop, Rock and Electronic (separate dataframes)\n",
    "Hiphop_10.drop(['genre_top'],axis=1,inplace=True)\n",
    "Rock_10.drop(['genre_top'],axis=1,inplace=True)\n",
    "Elec_10.drop(['genre_top'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hiphop_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make boxplot of variables for t10 data (data with all three genres) - before standardization \n",
    "t10.drop(['track_ID'], axis=1).astype(float).boxplot()\n",
    "plt.title('Boxplot of variables for dataset with three genres, before standardization')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Make boxplot of variables for t10 data (data with all three genres) - after standardization \n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "t10_scaled = scaler.fit_transform(t10.drop(['track_ID'], axis=1))\n",
    "t10_scaled = pd.DataFrame(t10_scaled)\n",
    "t10_scaled.rename(columns={'0': 'latitude', '1':'longitude'}, inplace=True)\n",
    "\n",
    "t10_scaled.astype(float).boxplot()\n",
    "plt.title('Boxplot of variables for dataset with three genres, after standardization')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make boxplot of variables for rock data - before standardization \n",
    "Rock_10.drop(['track_ID'], axis=1).astype(float).boxplot()\n",
    "plt.title('Boxplot of variables for dataset with Rock, before standardization')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Make boxplot of variables for rock data - after standardization \n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "Rock_10_scaled = scaler.fit_transform(Rock_10.drop(['track_ID'], axis=1))\n",
    "Rock_10_scaled = pd.DataFrame(Rock_10_scaled)\n",
    "Rock_10_scaled.rename(columns={'0': 'latitude', '1':'longitude'}, inplace=True)\n",
    "\n",
    "Rock_10_scaled.astype(float).boxplot()\n",
    "plt.title('Boxplot of variables for dataset with Rock, after standardization')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make boxplot of variables for hiphop data - before standardization \n",
    "Hiphop_10.drop(['track_ID'], axis=1).astype(float).boxplot()\n",
    "plt.title('Boxplot of variables for dataset with Hip-hop, before standardization')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Make boxplot of variables for hiphop data - after standardization \n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "Hiphop_10_scaled = scaler.fit_transform(Rock_10.drop(['track_ID'], axis=1))\n",
    "Hiphop_10_scaled = pd.DataFrame(Hiphop_10_scaled)\n",
    "Hiphop_10_scaled.rename(columns={'0': 'latitude', '1':'longitude'}, inplace=True)\n",
    "\n",
    "Hiphop_10_scaled.astype(float).boxplot()\n",
    "plt.title('Boxplot of variables for dataset with Hip-hop, after standardization')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make boxplot of variables for electronic data - before standardization \n",
    "Elec_10.drop(['track_ID'], axis=1).astype(float).boxplot()\n",
    "plt.title('Boxplot of variables for dataset with Electronic, before standardization')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Make boxplot of variables for electronic data - after standardization \n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()\n",
    "Elec_10_scaled = scaler.fit_transform(Elec_10.drop(['track_ID'], axis=1))\n",
    "Elec_10_scaled = pd.DataFrame(Elec_10_scaled)\n",
    "Elec_10_scaled.rename(columns={'0': 'latitude', '1':'longitude'}, inplace=True)\n",
    "\n",
    "Elec_10_scaled.astype(float).boxplot()\n",
    "plt.title('Boxplot of variables for dataset with Electronic, after standardization')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make boxplots of variables in genre dataset \n",
    "# Sa\n",
    "\n",
    "# Standardize variables as needed\n",
    "# Sa\n",
    "t10.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlation between variables with correlation plot \n",
    "# Kathy\n",
    "\n",
    "# Check for correlation between arist hotness, familiarity, discovery\n",
    "# Kathy\n",
    "rock10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make scatter plots of numerical variables versus outcome (track listens)\n",
    "# Rulan\n",
    "\n",
    "# an outlier on those categories\n",
    "\n",
    "\n",
    "rock10=t10[t10['genre_top']=='Rock']\n",
    "rockm=rock10.apply(pd.to_numeric, errors='coerce')\n",
    "rockm=rockm.drop(['track_ID','Rock','Hip-Hop','Electronic','genre_top'],axis=1)\n",
    "rock102=rockm.drop(['latitude','longitude','duration','bit_rate','artist_hotttnesss','artist_discovery','artist_familiarity'],axis=1)\n",
    "rock101=rockm[['track_listens','latitude','longitude','duration','bit_rate','artist_hotttnesss','artist_discovery','artist_familiarity']]\n",
    "\n",
    "\n",
    "scatter_matrix(rock101,figsize=(20,20))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(rock102,figsize=(30,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(rockm.track_listens,rockm.acousticness)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('duration')\n",
    "plt.title('track_listens v.s. duration')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rockm.track_listens,rockm.duration)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('acousticness')\n",
    "plt.title('track_listens v.s. acousticness')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rockm.track_listens,rockm.energy)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('energy')\n",
    "plt.title('track_listens v.s. energy')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rockm.track_listens,rockm.instrumentalness)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('instrumentalness')\n",
    "plt.title('track_listens v.s. instrumentalness')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rockm.track_listens,rockm.liveness)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('liveness')\n",
    "plt.title('track_listens v.s. liveness')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rockm.track_listens,rockm.speechiness)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('speechiness')\n",
    "plt.title('track_listens v.s. speechiness')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(rockm.track_listens,rockm.tempo)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('tempo')\n",
    "plt.title('track_listens v.s. tempo')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correlation plots for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model - use the year with most songs data and include genre as a categorical variable (use the same top 3 genres)\n",
    "\n",
    "# Convert genre to dummy variables\n",
    "\n",
    "# Use linear regression with same variables as above (sentiment analysis, track metadata, echonest named variable, location)\n",
    "\n",
    "# IDEA: we will see that genre is a big predictor of track listens - thus, we train separate models for each genre to dig deeper into why that is\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = t10[['popular']].values\n",
    "X=t10.drop(['track_listens','track_ID','popular'],axis=1).values\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "#y_scaled = scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=7)\n",
    "\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_train, y_train)\n",
    "\n",
    "linear_score=linear.score(X_test, y_test)\n",
    "\n",
    "print(linear_score)\n",
    "\n",
    "importances = pd.DataFrame({'feature':t10.drop(['track_listens','track_ID','popular'],axis=1).columns,'importance':linear.coef_[0]})\n",
    "plt.bar(linear.coef_[0])\n",
    "plt.xticks(np.arange(len(regr.coef_[0])),t10.drop(['track_listens','track_ID','popular'],axis=1).columns, rotation=90)\n",
    "print(importances.sort_values(by='importance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model - use the year with most songs data and include genre as a categorical variable (use the same top 3 genres)\n",
    "\n",
    "# Convert genre to dummy variables\n",
    "\n",
    "# Use linear regression with same variables as above (sentiment analysis, track metadata, echonest named variable, location)\n",
    "\n",
    "# IDEA: we will see that genre is a big predictor of track listens - thus, we train separate models for each genre to dig deeper into why that is\n",
    "\n",
    "# TRY CLASSIFICATION - songs with top 10% track listens are POPULAR, else NOT POPULAR\n",
    "\n",
    "# Convert track listens to categorical (if track_listens > 1147.25, then = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting ROC\n",
    "def generate_ROCplot(fpr,tpr,label,roc_auc):\n",
    "    plt.clf()\n",
    "    plt.plot(fpr, tpr, '.-',label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression for popularity\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = t10['popular']\n",
    "X=t10.drop(['track_listens','popular', 'track_ID'],axis=1).values\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "#y_scaled = scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "\n",
    "\n",
    "regr = LogisticRegression(penalty='l1')\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "#compute AUC ROC\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#compute ROC \n",
    "probas_ = regr.fit(X_train,y_train).predict_proba(X_test)\n",
    "\n",
    "# Generate ROC  for LR with l1 penalty\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc)\n",
    "\n",
    "# Plots ROC\n",
    "generate_ROCplot(fpr,tpr,'LR',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coefficient importance\n",
    "plt.bar(np.arange(len(regr.coef_[0])),regr.coef_[0])\n",
    "plt.xticks(np.arange(len(regr.coef_[0])), ['latitude', 'longitude', 'bit_rate', 'duration',\n",
    "       'acousticness', 'danceability', 'energy',\n",
    "       'instrumentalness', 'liveness', 'speechiness', 'tempo', 'valence',\n",
    "       'artist_hotttnesss', 'artist_discovery', 'artist_familiarity',\n",
    "       'senti neg', 'senti pos', 'Rock', 'Hip-Hop', 'Electronic'], rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 5-folds, split data into training and testing (one of the genres) - will repeat for other two genres (maybe write function to do this)\n",
    "\n",
    "# Train linear regression model using all variables\n",
    "\n",
    "# Regularize using L^1 penalty due to large number of features - pick optimal penalty and compare R^2\n",
    "\n",
    "# Use GridSearchCV with at least 2-fold validation\n",
    "\n",
    "# Check resulting model on 5 random folds of data \n",
    "\n",
    "# Plot coefficients for resulting model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train random forest regression model \n",
    "\n",
    "# Regularize using L^1 penalty due to large number of features - pick optimal penalty and compare R^2\n",
    "\n",
    "# Use GridSearchCV with at least 2-fold validation\n",
    "\n",
    "# Check resulting model on 5 random folds of data \n",
    "\n",
    "# Plot coefficients for resulting model \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = Rock_10[['popular']].values\n",
    "X=Rock_10.drop(['track_listens','track_ID'],axis=1).values\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "#y_scaled = scaler.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=7)\n",
    "\n",
    "\n",
    "random_forest = RandomForestRegressor(n_estimators=100)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "Y_prediction = random_forest.predict(X_test)\n",
    "\n",
    "random_forest.score(X_test, y_test)\n",
    "acc_random_forest = random_forest.score(X_test, y_test) * 100\n",
    "print(acc_random_forest)\n",
    "importances = pd.DataFrame({'feature':Rock_10.drop(['track_listens','track_ID'],axis=1).columns,'importance':random_forest.feature_importances_})\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "importances.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore adding other features to model, such as nonlinear features (interaction terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jazz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hip hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare how models are different between different genres - for instance, if certain variables are more or less important for different genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function that takes in optimal_model, new song audio and necessary info (like genre, track title, etc.) and uses librosa to extract features\n",
    "# and output a prediction of number of track listens \n",
    "\n",
    "# for testing a new song, im not sure if our model gonna use \"features\", so i'll also look for data in \"echonest \"\n",
    "\n",
    "# librosa code only extract same data on features\n",
    "\n",
    "# Load the example clip\n",
    "# Load 30 seconds of a wav file, starting 15 seconds in\n",
    "y, sr = librosa.load('blahblah.mp3', offset=15.0, duration=30.0)\n",
    "\n",
    "# Set the hop length; at 22050 Hz, 512 samples ~= 23ms\n",
    "hop_length = 512\n",
    "\n",
    "# Separate harmonics and percussives into two waveforms\n",
    "y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "\n",
    "# Beat track on the percussive signal\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y_percussive,\n",
    "                                             sr=sr)\n",
    "\n",
    "# Compute MFCC features from the raw signal\n",
    "mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
    "\n",
    "# And the first-order differences (delta features)\n",
    "mfcc_delta = librosa.feature.delta(mfcc)\n",
    "\n",
    "# Stack and synchronize between beat events\n",
    "# This time, we'll use the mean value (default) instead of median\n",
    "beat_mfcc_delta = librosa.util.sync(np.vstack([mfcc, mfcc_delta]),\n",
    "                                    beat_frames)\n",
    "\n",
    "# Compute chroma features from the harmonic signal\n",
    "chromagram = librosa.feature.chroma_cqt(y=y_harmonic,\n",
    "                                        sr=sr)\n",
    "\n",
    "# Aggregate chroma features between beat events\n",
    "# We'll use the median value of each feature between beat frames\n",
    "beat_chroma = librosa.util.sync(chromagram,\n",
    "                                beat_frames,\n",
    "                                aggregate=np.median)\n",
    "\n",
    "# Finally, stack all beat-synchronous features together\n",
    "beat_features = np.vstack([beat_chroma, beat_mfcc_delta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Statistic for each feature\n",
    "def columns():\n",
    "    feature_sizes = dict(chroma_stft=12, chroma_cqt=12, chroma_cens=12,\n",
    "                         tonnetz=6, mfcc=20, rmse=1, zcr=1,\n",
    "                         spectral_centroid=1, spectral_bandwidth=1,\n",
    "                         spectral_contrast=7, spectral_rolloff=1)\n",
    "    moments = ('mean', 'std', 'skew', 'kurtosis', 'median', 'min', 'max')\n",
    "\n",
    "    columns = []\n",
    "    for name, size in feature_sizes.items():\n",
    "        for moment in moments:\n",
    "            it = ((name, moment, '{:02d}'.format(i+1)) for i in range(size))\n",
    "            columns.extend(it)\n",
    "\n",
    "    names = ('feature', 'statistics', 'number')\n",
    "    columns = pd.MultiIndex.from_tuples(columns, names=names)\n",
    "\n",
    "    # More efficient to slice if indexes are sorted.\n",
    "    return columns.sort_values()\n",
    "\n",
    "\n",
    "def feature_stats(name, values):\n",
    "    features[name, 'mean'] = np.mean(values, axis=1)\n",
    "    features[name, 'std'] = np.std(values, axis=1)\n",
    "    features[name, 'skew'] = stats.skew(values, axis=1)\n",
    "    features[name, 'kurtosis'] = stats.kurtosis(values, axis=1)\n",
    "    features[name, 'median'] = np.median(values, axis=1)\n",
    "    features[name, 'min'] = np.min(values, axis=1)\n",
    "    features[name, 'max'] = np.max(values, axis=1)\n",
    "\n",
    "\n",
    "features = pd.Series(index=columns(), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "y, sr = librosa.load('xxx.mp3', offset=15.0, duration=30.0)\n",
    "\n",
    "stft=np.abs(librosa.stft(y, n_fft=2048, hop_length=512))\n",
    "f = librosa.feature.chroma_stft(S=stft**2, n_chroma=12)\n",
    "feature_stats('chroma_stft', f)\n",
    "\n",
    "f = librosa.feature.zero_crossing_rate(y, frame_length=2048, hop_length=512)\n",
    "feature_stats('zcr', f)\n",
    "\n",
    "cqt = np.abs(librosa.cqt(y, sr=sr, hop_length=512, bins_per_octave=12,\n",
    "                                 n_bins=7*12, tuning=None))\n",
    "assert cqt.shape[0] == 7 * 12\n",
    "assert np.ceil(len(y)/512) <= cqt.shape[1] <= np.ceil(len(y)/512)+1\n",
    "\n",
    "\n",
    "f = librosa.feature.chroma_cqt(C=cqt, n_chroma=12, n_octaves=7)\n",
    "feature_stats('chroma_cqt', f)\n",
    "\n",
    "f = librosa.feature.chroma_cens(C=cqt, n_chroma=12, n_octaves=7)\n",
    "feature_stats('chroma_cens', f)\n",
    "\n",
    "f = librosa.feature.tonnetz(chroma=f)\n",
    "feature_stats('tonnetz', f)\n",
    "\n",
    "print(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

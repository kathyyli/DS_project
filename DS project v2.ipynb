{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import sklearn as skl\n",
    "import pandas as pd\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import pylab as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "#import librosa  #need install first\n",
    "#import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qusac\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (0,1,5,6,8,12,18,20,21,22,24,33,34,38,39,44,47,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\qusac\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,11,13,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Load metadata and features.\n",
    "tracks = pd.read_csv('tracks.csv', header=None)\n",
    "echonest = pd.read_csv('echonest.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column names from \"track.1\", ... to named columns\n",
    "header = tracks.iloc[1]\n",
    "header[0] = 'track_ID'\n",
    "tracks.drop(tracks.index[[0,1,2]], inplace=True)\n",
    "tracks.rename(columns=header, inplace=True)\n",
    "tracks.head()\n",
    "\n",
    "header = echonest.iloc[2]\n",
    "header[0]='track_ID'\n",
    "echonest.drop(echonest.index[[0,1,2,3]],inplace=True)\n",
    "echonest.rename(columns = header,inplace=True)\n",
    "echonest.head()\n",
    "\n",
    "# Only select songs for which we have echonest data\n",
    "tracks_with_echonest_data = tracks[tracks['track_ID'].isin(echonest['track_ID'])]\n",
    "echonest_data = echonest[echonest['track_ID'].isin(tracks_with_echonest_data['track_ID'])]\n",
    "\n",
    "merged_echonest_data = pd.merge(tracks_with_echonest_data, echonest_data, on = 'track_ID')\n",
    "\n",
    "# Change duplicate \"listens\" column to track listens and album listens\n",
    "duplicate_listens = {'listens': ['album_listens', 'track_listens']}\n",
    "\n",
    "merged_echonest_data = merged_echonest_data.rename(columns=lambda c: duplicate_listens[c].pop(0) if c in duplicate_listens.keys() else c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({nan: 3140, '2010': 1461, '2009': 1426, '2011': 1101, '2008': 1088, '2007': 464, '2012': 380, '2006': 346, '2013': 298, '2014': 245, '2004': 167, '2005': 150, '2015': 145, '2003': 94, '2002': 84, '2001': 69, '1999': 68, '2000': 34, '1997': 25, '1995': 20, '1982': 19, '1998': 17, '1996': 10, '1986': 10, '1981': 6, '1973': 5, '1985': 5, '1992': 3, '1913': 2, '1976': 2, '1912': 1, '1916': 1, '1911': 1, '1909': 1, '1905': 1, '1907': 1, '1988': 1, '1902': 1})\n",
      "dict_values([1426, 1088, 464, 150, 20, 346, 3140, 94, 19, 6, 69, 84, 167, 34, 68, 10, 1, 1, 1, 2, 1, 1, 1, 25, 3, 1, 17, 1461, 2, 10, 1101, 5, 245, 380, 298, 5, 1, 145])\n",
      "dict_keys(['2009', '2008', '2007', '2005', '1995', '2006', nan, '2003', '1982', '1981', '2001', '2002', '2004', '2000', '1999', '1996', '1912', '1916', '1911', '1913', '1909', '1905', '1907', '1997', '1992', '1988', '1998', '2010', '1976', '1986', '2011', '1973', '2014', '2012', '2013', '1985', '1902', '2015'])\n",
      "[(nan, 3140), ('2010', 1461), ('2009', 1426)]\n"
     ]
    }
   ],
   "source": [
    "# Extract year released from release date variable\n",
    "album_release_year = []\n",
    "\n",
    "for x in merged_echonest_data['date_released']:\n",
    "    if type(x)==str:\n",
    "        album_release_year.append(x[0:4])\n",
    "    else: \n",
    "        album_release_year.append(np.nan)\n",
    "\n",
    "#add album release year to dataframe\n",
    "merged_echonest_data['album_release_year'] = album_release_year\n",
    "\n",
    "# Check number of songs released per year and pick year with maximum\n",
    "import collections\n",
    "counter=collections.Counter(album_release_year)\n",
    "print(counter)\n",
    "print(counter.values())\n",
    "print(counter.keys())\n",
    "print(counter.most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({nan: 524, 'Rock': 372, 'Electronic': 271, 'Hip-Hop': 133, 'Folk': 67, 'Pop': 58, 'Jazz': 23, 'International': 12, 'Instrumental': 1})\n",
      "dict_values([23, 372, 58, 271, 524, 133, 67, 12, 1])\n",
      "dict_keys(['Jazz', 'Rock', 'Pop', 'Electronic', nan, 'Hip-Hop', 'Folk', 'International', 'Instrumental'])\n",
      "[(nan, 524), ('Rock', 372), ('Electronic', 271), ('Hip-Hop', 133), ('Folk', 67), ('Pop', 58), ('Jazz', 23), ('International', 12), ('Instrumental', 1)]\n"
     ]
    }
   ],
   "source": [
    "# For chosen year, check the number of songs released per genre; pick top three genres to use\n",
    "tracks_2010 = merged_echonest_data[merged_echonest_data['album_release_year'] == '2010']\n",
    "\n",
    "# Top genres for 2010\n",
    "counter=collections.Counter(tracks_2010['genre_top'])\n",
    "print(counter)\n",
    "print(counter.values())\n",
    "print(counter.keys())\n",
    "print(counter.most_common(10))\n",
    "\n",
    "# Remove NAs, if needed \n",
    "\n",
    "# Select sensical variables from merged dataset (i.e., some track metadata and named echonest features)\n",
    "\n",
    "# Stuff above - Kathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qusac\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\qusac\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/Users/rulanxiao/Desktop/fma_metadata/tracks.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9fb553d63f89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vader_lexicon'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtracks_senti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/rulanxiao/Desktop/fma_metadata/tracks.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtracks_senti\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'/Users/rulanxiao/Desktop/fma_metadata/tracks.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Add sentiment analysis by title into dataset - compound score of positive / negative sentiment for song title\n",
    "# Rulan\n",
    "# require to install nltk first\n",
    "# this part work nice, file will be generated to our project folder. \n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "tracks_senti = pd.read_csv('/Users/rulanxiao/Desktop/fma_metadata/tracks.csv',header=None)\n",
    "\n",
    "header = tracks_senti.iloc[1]\n",
    "header[0]='track ID'\n",
    "header[52]='track title'\n",
    "tracks_senti.drop(tracks_senti.index[[0,1,2]],inplace=True)\n",
    "tracks_senti.rename(columns = header,inplace=True)\n",
    "tracks_senti.head()\n",
    "\n",
    "df=tracks_senti[['track ID','track title']]\n",
    "df.dropna(axis=0, how='any')\n",
    "\n",
    "ml = df[\"track title\"].values\n",
    "title=[]\n",
    "for i in range(len(ml)):\n",
    "    a=str(ml[i])\n",
    "    title.append(a)\n",
    "idd = df[\"track ID\"].values\n",
    "\n",
    "neg=[]\n",
    "neu=[]\n",
    "pos=[]\n",
    "comp=[]\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in title:\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    score=[]\n",
    "    for k in ss:\n",
    "        a=ss[k]\n",
    "        score.append(a)\n",
    "    neg.append(score[0])\n",
    "    neu.append(score[1])\n",
    "    pos.append(score[2])\n",
    "    comp.append(score[3])\n",
    "\n",
    "sentimentall = pd.DataFrame({'track_ID':idd,'track_title':title,'senti neg': neg,'senti neu': neu,'senti pos': pos,'senti comp': comp})\n",
    "\n",
    "sentimentall.to_csv('sentimental_analysis_title.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment= pd.read_csv('sentimental_analysis_title.csv', header=[0])\n",
    "#sentiment.rename(index=str, columns={'track_ID' : 'track ID'},inplace=True)\n",
    "sentiment=sentiment[['track_ID','senti comp', 'senti neg','senti pos']]\n",
    "tracks_2010 = pd.merge(tracks_2010, sentiment, how='inner', on=['track_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qusac\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\qusac\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\qusac\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_ID</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>bit_rate</th>\n",
       "      <th>duration</th>\n",
       "      <th>genre_top</th>\n",
       "      <th>track_listens</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>...</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>artist_hotttnesss</th>\n",
       "      <th>artist_discovery</th>\n",
       "      <th>artist_familiarity</th>\n",
       "      <th>senti neg</th>\n",
       "      <th>senti pos</th>\n",
       "      <th>Rock</th>\n",
       "      <th>Hip-Hop</th>\n",
       "      <th>Electronic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26531</td>\n",
       "      <td>55.3231</td>\n",
       "      <td>23.8959</td>\n",
       "      <td>320000</td>\n",
       "      <td>324</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>1532</td>\n",
       "      <td>0.981185</td>\n",
       "      <td>0.462934</td>\n",
       "      <td>0.113998</td>\n",
       "      <td>...</td>\n",
       "      <td>73.941</td>\n",
       "      <td>0.273272</td>\n",
       "      <td>0.266514</td>\n",
       "      <td>0.255115</td>\n",
       "      <td>0.271908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26532</td>\n",
       "      <td>55.3231</td>\n",
       "      <td>23.8959</td>\n",
       "      <td>320000</td>\n",
       "      <td>431</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>1506</td>\n",
       "      <td>0.988642</td>\n",
       "      <td>0.421825</td>\n",
       "      <td>0.210379</td>\n",
       "      <td>...</td>\n",
       "      <td>125.2</td>\n",
       "      <td>0.117733</td>\n",
       "      <td>0.266514</td>\n",
       "      <td>0.255115</td>\n",
       "      <td>0.271908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26533</td>\n",
       "      <td>55.3231</td>\n",
       "      <td>23.8959</td>\n",
       "      <td>320000</td>\n",
       "      <td>688</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>359</td>\n",
       "      <td>0.992052</td>\n",
       "      <td>0.449904</td>\n",
       "      <td>0.187091</td>\n",
       "      <td>...</td>\n",
       "      <td>82.752</td>\n",
       "      <td>0.143674</td>\n",
       "      <td>0.266514</td>\n",
       "      <td>0.255115</td>\n",
       "      <td>0.271908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26534</td>\n",
       "      <td>55.3231</td>\n",
       "      <td>23.8959</td>\n",
       "      <td>320000</td>\n",
       "      <td>782</td>\n",
       "      <td>Jazz</td>\n",
       "      <td>381</td>\n",
       "      <td>0.988513</td>\n",
       "      <td>0.299413</td>\n",
       "      <td>0.212382</td>\n",
       "      <td>...</td>\n",
       "      <td>74.872</td>\n",
       "      <td>0.126995</td>\n",
       "      <td>0.266514</td>\n",
       "      <td>0.255115</td>\n",
       "      <td>0.271908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27223</td>\n",
       "      <td>41.8719</td>\n",
       "      <td>12.5674</td>\n",
       "      <td>320000</td>\n",
       "      <td>209</td>\n",
       "      <td>Rock</td>\n",
       "      <td>260</td>\n",
       "      <td>0.403708</td>\n",
       "      <td>0.679438</td>\n",
       "      <td>0.732271</td>\n",
       "      <td>...</td>\n",
       "      <td>124.293</td>\n",
       "      <td>0.826063</td>\n",
       "      <td>0.414522</td>\n",
       "      <td>0.396793</td>\n",
       "      <td>0.296588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  track_ID latitude longitude bit_rate duration genre_top track_listens  \\\n",
       "0    26531  55.3231   23.8959   320000      324      Jazz          1532   \n",
       "1    26532  55.3231   23.8959   320000      431      Jazz          1506   \n",
       "2    26533  55.3231   23.8959   320000      688      Jazz           359   \n",
       "3    26534  55.3231   23.8959   320000      782      Jazz           381   \n",
       "4    27223  41.8719   12.5674   320000      209      Rock           260   \n",
       "\n",
       "  acousticness danceability    energy     ...        tempo   valence  \\\n",
       "0     0.981185     0.462934  0.113998     ...       73.941  0.273272   \n",
       "1     0.988642     0.421825  0.210379     ...        125.2  0.117733   \n",
       "2     0.992052     0.449904  0.187091     ...       82.752  0.143674   \n",
       "3     0.988513     0.299413  0.212382     ...       74.872  0.126995   \n",
       "4     0.403708     0.679438  0.732271     ...      124.293  0.826063   \n",
       "\n",
       "  artist_hotttnesss artist_discovery artist_familiarity senti neg senti pos  \\\n",
       "0          0.266514         0.255115           0.271908       0.0       0.0   \n",
       "1          0.266514         0.255115           0.271908       0.0       0.0   \n",
       "2          0.266514         0.255115           0.271908       0.0       0.0   \n",
       "3          0.266514         0.255115           0.271908       0.0       0.0   \n",
       "4          0.414522         0.396793           0.296588       0.0       0.0   \n",
       "\n",
       "  Rock  Hip-Hop  Electronic  \n",
       "0    0        0           0  \n",
       "1    0        0           0  \n",
       "2    0        0           0  \n",
       "3    0        0           0  \n",
       "4    1        0           0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For baseline model - add genre as a categorical variable \n",
    "# Sa\n",
    "\n",
    "t10 = tracks_2010[['track_ID','latitude','longitude','bit_rate','duration','genre_top','track_listens','acousticness','danceability','energy','instrumentalness','liveness','speechiness','tempo','valence','artist_hotttnesss','artist_discovery','artist_familiarity', 'senti neg','senti pos']]\n",
    "t10['Rock'] = (t10['genre_top'] == 'Rock').astype(int)\n",
    "t10['Hip-Hop'] = (t10['genre_top'] == 'Hip-Hop').astype(int)\n",
    "t10['Electronic'] = (t10['genre_top'] == 'Electronic').astype(int)\n",
    "\n",
    "# For genre-specific models - split dataset into three based on top three genres\n",
    "# Sa\n",
    "\n",
    "t10.groupby(['genre_top']).size()\n",
    "Hiphop_10=t10.loc[t10['genre_top'] == 'Hip-Hop']\n",
    "Rock_10=t10.loc[t10['genre_top'] == 'Rock']\n",
    "Elec_10=t10.loc[t10['genre_top'] == 'Electronic']\n",
    "\n",
    "\n",
    "\n",
    "t10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qusac\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "t10['g']=t10['Rock']+ t10['Hip-Hop'] +t10['Electronic']\n",
    "t10=t10.loc[t10['g'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t10.drop(['g'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qusac\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2be0f551f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make boxplots of variables in genre dataset \n",
    "# Sa\n",
    "t10[['duration','acousticness','danceability']].boxplot()\n",
    "# Standardize variables as needed\n",
    "# Sa\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = np.array(t10[['track_listens']].values)\n",
    "X=(t10[['bit_rate','duration','track_listens','acousticness','danceability','energy','instrumentalness','liveness','speechiness','tempo','valence','artist_hotttnesss', 'senti neg','senti pos']].values)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1))\n",
    "# Repeat for other two genres\n",
    "# Sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlation between variables with correlation plot \n",
    "# Kathy\n",
    "\n",
    "# Check for correlation between arist hotness, familiarity, discovery\n",
    "# Kathy\n",
    "t10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scatter plots of numerical variables versus outcome (track listens)\n",
    "# Rulan\n",
    "\n",
    "plt.scatter(tracks.track_listens,tracks.acousticness)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('duration')\n",
    "plt.title('track_listens v.s. duration')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(t10.track_listens,t10.acousticness)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('acousticness')\n",
    "plt.title('track_listens v.s. acousticness')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(t10.track_listens,t10.acousticness)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('energy')\n",
    "plt.title('track_listens v.s. energy')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(t10.track_listens,t10.acousticness)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('instrumentalness')\n",
    "plt.title('track_listens v.s. instrumentalness')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(t10.track_listens,t10.acousticness)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('liveness')\n",
    "plt.title('track_listens v.s. liveness')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(t10.track_listens,t10.acousticness)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('speechiness')\n",
    "plt.title('track_listens v.s. speechiness')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(t10.track_listens,t10.acousticness)\n",
    "plt.ylabel('track_listens')\n",
    "plt.xlabel('tempo')\n",
    "plt.title('track_listens v.s. tempo')\n",
    "plt.show()\n",
    "\n",
    "scatter_matrix(??,figsize=(30,30))\n",
    "plt.show()\n",
    "\n",
    "# Correlation plots for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model - use the year with most songs data and include genre as a categorical variable (use the same top 3 genres)\n",
    "\n",
    "# Convert genre to dummy variables\n",
    "\n",
    "# Use linear regression with same variables as above (sentiment analysis, track metadata, echonest named variable, location)\n",
    "\n",
    "# IDEA: we will see that genre is a big predictor of track listens - thus, we train separate models for each genre to dig deeper into why that is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 5-folds, split data into training and testing (one of the genres) - will repeat for other two genres (maybe write function to do this)\n",
    "\n",
    "# Train linear regression model using all variables\n",
    "\n",
    "# Regularize using L^1 penalty due to large number of features - pick optimal penalty and compare R^2\n",
    "\n",
    "# Use GridSearchCV with at least 2-fold validation\n",
    "\n",
    "# Check resulting model on 5 random folds of data \n",
    "\n",
    "# Plot coefficients for resulting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train random forest regression model \n",
    "\n",
    "# Regularize using L^1 penalty due to large number of features - pick optimal penalty and compare R^2\n",
    "\n",
    "# Use GridSearchCV with at least 2-fold validation\n",
    "\n",
    "# Check resulting model on 5 random folds of data \n",
    "\n",
    "# Plot coefficients for resulting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore adding other features to model, such as nonlinear features (interaction terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jazz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hip hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare how models are different between different genres - for instance, if certain variables are more or less important for different genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function that takes in optimal_model, new song audio and necessary info (like genre, track title, etc.) and uses librosa to extract features\n",
    "# and output a prediction of number of track listens \n",
    "\n",
    "# Load the example clip\n",
    "# Load 30 seconds of a wav file, starting 15 seconds in\n",
    "y, sr = librosa.load('blahblah.mp3', offset=15.0, duration=30.0)\n",
    "\n",
    "# Set the hop length; at 22050 Hz, 512 samples ~= 23ms\n",
    "hop_length = 512\n",
    "\n",
    "# Separate harmonics and percussives into two waveforms\n",
    "y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "\n",
    "# Beat track on the percussive signal\n",
    "tempo, beat_frames = librosa.beat.beat_track(y=y_percussive,\n",
    "                                             sr=sr)\n",
    "\n",
    "# Compute MFCC features from the raw signal\n",
    "mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
    "\n",
    "# And the first-order differences (delta features)\n",
    "mfcc_delta = librosa.feature.delta(mfcc)\n",
    "\n",
    "# Stack and synchronize between beat events\n",
    "# This time, we'll use the mean value (default) instead of median\n",
    "beat_mfcc_delta = librosa.util.sync(np.vstack([mfcc, mfcc_delta]),\n",
    "                                    beat_frames)\n",
    "\n",
    "# Compute chroma features from the harmonic signal\n",
    "chromagram = librosa.feature.chroma_cqt(y=y_harmonic,\n",
    "                                        sr=sr)\n",
    "\n",
    "# Aggregate chroma features between beat events\n",
    "# We'll use the median value of each feature between beat frames\n",
    "beat_chroma = librosa.util.sync(chromagram,\n",
    "                                beat_frames,\n",
    "                                aggregate=np.median)\n",
    "\n",
    "# Finally, stack all beat-synchronous features together\n",
    "beat_features = np.vstack([beat_chroma, beat_mfcc_delta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Statistic for each feature\n",
    "def columns():\n",
    "    feature_sizes = dict(chroma_stft=12, chroma_cqt=12, chroma_cens=12,\n",
    "                         tonnetz=6, mfcc=20, rmse=1, zcr=1,\n",
    "                         spectral_centroid=1, spectral_bandwidth=1,\n",
    "                         spectral_contrast=7, spectral_rolloff=1)\n",
    "    moments = ('mean', 'std', 'skew', 'kurtosis', 'median', 'min', 'max')\n",
    "\n",
    "    columns = []\n",
    "    for name, size in feature_sizes.items():\n",
    "        for moment in moments:\n",
    "            it = ((name, moment, '{:02d}'.format(i+1)) for i in range(size))\n",
    "            columns.extend(it)\n",
    "\n",
    "    names = ('feature', 'statistics', 'number')\n",
    "    columns = pd.MultiIndex.from_tuples(columns, names=names)\n",
    "\n",
    "    # More efficient to slice if indexes are sorted.\n",
    "    return columns.sort_values()\n",
    "\n",
    "\n",
    "def feature_stats(name, values):\n",
    "    features[name, 'mean'] = np.mean(values, axis=1)\n",
    "    features[name, 'std'] = np.std(values, axis=1)\n",
    "    features[name, 'skew'] = stats.skew(values, axis=1)\n",
    "    features[name, 'kurtosis'] = stats.kurtosis(values, axis=1)\n",
    "    features[name, 'median'] = np.median(values, axis=1)\n",
    "    features[name, 'min'] = np.min(values, axis=1)\n",
    "    features[name, 'max'] = np.max(values, axis=1)\n",
    "\n",
    "\n",
    "features = pd.Series(index=columns(), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##OTHER FEATURES\n",
    "y, sr = librosa.load('Rough & Laugh.mp3', offset=15.0, duration=30.0)\n",
    "\n",
    "stft=np.abs(librosa.stft(y, n_fft=2048, hop_length=512))\n",
    "f = librosa.feature.chroma_stft(S=stft**2, n_chroma=12)\n",
    "feature_stats('chroma_stft', f)\n",
    "\n",
    "f = librosa.feature.zero_crossing_rate(y, frame_length=2048, hop_length=512)\n",
    "feature_stats('zcr', f)\n",
    "\n",
    "cqt = np.abs(librosa.cqt(y, sr=sr, hop_length=512, bins_per_octave=12,\n",
    "                                 n_bins=7*12, tuning=None))\n",
    "assert cqt.shape[0] == 7 * 12\n",
    "assert np.ceil(len(y)/512) <= cqt.shape[1] <= np.ceil(len(y)/512)+1\n",
    "\n",
    "\n",
    "f = librosa.feature.chroma_cqt(C=cqt, n_chroma=12, n_octaves=7)\n",
    "feature_stats('chroma_cqt', f)\n",
    "\n",
    "f = librosa.feature.chroma_cens(C=cqt, n_chroma=12, n_octaves=7)\n",
    "feature_stats('chroma_cens', f)\n",
    "\n",
    "f = librosa.feature.tonnetz(chroma=f)\n",
    "feature_stats('tonnetz', f)\n",
    "\n",
    "print(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
